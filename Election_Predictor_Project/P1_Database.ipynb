{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Database construction and cleaning\n",
    "\n",
    "## Executive summary: \n",
    "\n",
    "**Objective:**\n",
    "\n",
    "* My objective for this project is to construct a model that predicts whether an Australian Federeal Electorate will tend to vote right or left. This model will use a variety of demographic and some voting statistics to find which features can predict left or right leaningness. \n",
    "\n",
    "\n",
    "* In order to do this, I must collect comprehensive demographic and election data. The best sources for these are the Australian Electoral Commission (AEC) for election data, and the Australian Bureau of Statistics (ABS) for census data. This data will also need to be grouped by each electorate and will need to be joined by the election year and each individual electorate. \n",
    "\n",
    "\n",
    "* Because the data will be very fragmented and made up of many individual CSVs, I will need to store it in my own data base and organise it so every individual category and table relates to each other by its individual electorate identifier and its relevant election year.\n",
    "\n",
    "\n",
    "* This will be done via PostgreSQL, and I'll be using psycopg2 to connect and write functions that will automatically clean my CSVs and upload them to my data base.\n",
    "\n",
    "**Getting the data and data sources:**\n",
    "\n",
    "* **Election data** - https://results.aec.gov.au/ \n",
    "\n",
    "    * Election data was relatively easy to download, with only 5 CSVs per election, in very easy to access downloadable files. In total there were 35 CSVs, 5 categories for the past 7 election years.\n",
    "\n",
    "* **Census data** - https://www.abs.gov.au/websitedbs/censushome.nsf/home/historicaldata\n",
    "\n",
    "    * Census and demographics data was much harder to download and build. Firstly, raw census data is only available for the 2016 and 2011 census and nothing before. It was crucial to my goals to have as many demographics data points as possible, so I wanted to encapsulate as much census data as possible.\n",
    "    \n",
    "    * Unfortunately, it was impossible to have any sort of meaningful statistics for any census data before 2006, so I had to settle with three demographics data points, the 2016, 2011, and 2006 Census'. \n",
    "    \n",
    "    * Because 2006 did not have any available raw data, I had to resort to using the ABS' table builder. Table builder essentially allows the user to access the ABS' database and construct your own tables and download them as a CSV. \n",
    "    * Unfortunately, there was no way I could automate the process of building every individual table for every demographic category I wanted, so I had to manually construct 81 CSVs using the ABS' table builder. 27 categories for every census across the 3 different census years\n",
    "    \n",
    "## Data base construction:\n",
    "\n",
    "* With the CSVs now downloaded and constructed, it was time to build the database that will store all the data from all the CSVs. \n",
    "\n",
    "**Challenges**\n",
    "\n",
    "* The main challenges presented here are mostly to with the Census data. Some of the Census questions will differ slightly from one another, so I will need to go through every single CSV and see if any of the column questions aren't the same, and if they aren't, I will need to align them so they're all identical. \n",
    "\n",
    "\n",
    "* Similarly, I will also need to align income, family income, rent and mortgage brackets. Since the Census' span across a decade, some of the financial figures will need to be adjusted for inflation, and some of the income brackets will need to be rolled over to align with one another.\n",
    "\n",
    "\n",
    "* The other thing I will need to deal with is turning all the raw counts into proportions. So I will need to write a function that automatically converts any count into a proportion (dividing it by the total figure). This will need to be done for every single cell in every single CSV.\n",
    "\n",
    "\n",
    "* The column names will need to be adjusted so they are suitable for SQL naming conventions, this includes removing any special characters and not starting any column with a number.\n",
    "\n",
    "\n",
    "* Because there are 7 election years and only 3 census years, I will need to find a way to align them. To do this I will interpolate the census data using a carried annual growth rate, calculated from the growth over 5 years between each census. While not perfect, this will essentially fill in the gaps and align the census data to each election year, making a much more accurate comparison.\n",
    "\n",
    "**Below I will begin with the database construction process:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing various modules that I need for my data base construction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#This module will allow me to interact and execute queries via python on my postgresql data base\n",
    "import psycopg2\n",
    "#This module allows me to read in and edit multiple files in a directory\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "from sqlalchemy import create_engine\n",
    "#My custom script containing all methods I will be using for wrangling and sorting\n",
    "from cowboy import wrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting display options for the data frame\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.min_rows',20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing election data from the AEC and putting it into SQL:**\n",
    "\n",
    "* As I am just starting out, I will manually construct the SQL tables so that data can be inserted into them. This is only for the AEC data, as there is not a lot of it and it's pretty straight forward with the naming conventions. Minimal manipulation is needed for the actual data, so doing it manually is enough for me now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to my local database on Postgresql\n",
    "conn_str = \"host='localhost' \\\n",
    "dbname='Election_DB' \\\n",
    "user='postgres' \\\n",
    "password='1234'\"\n",
    "\n",
    "conn = psycopg2.connect(conn_str)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually creating the scaffolding tables so I can insert the csvs later\n",
    "\n",
    "#Creating a string of a SQL query that names the table and assigns the data type to each column manually\n",
    "exe_enrolment = \"\"\"\n",
    "CREATE TABLE GeneralEnrolment (\n",
    "    DivisionID int,\n",
    "    DivisionNm text,\n",
    "    StateAb text,\n",
    "    CloseOfRollsEnrolment int,\n",
    "    NotebookRollAdditions int,\n",
    "    NotebookRollDeletions int,\n",
    "    ReinstatementsPostal int,\n",
    "    ReinstatementsPrePoll int,\n",
    "    ReinstatementsAbsent int,\n",
    "    ReinstatementsProvisional int,\n",
    "    Enrolment int,\n",
    "    Election_Year int)\n",
    "    \"\"\"\n",
    "\n",
    "exe_membelect = \"\"\"\n",
    "CREATE TABLE HouseMembersElected (\n",
    "    DivisionID int,\n",
    "    DivisionNm text,\n",
    "    StateAb text,\n",
    "    CandidateID int,\n",
    "    GivenNm text,\n",
    "    Surname text,\n",
    "    PartyNm text,\n",
    "    PartyAb text,\n",
    "    Election_Year int)\n",
    "    \"\"\"\n",
    "\n",
    "exe_tpp = \"\"\"\n",
    "CREATE TABLE HouseTpp (\n",
    "    DivisionNm text,\n",
    "    DivisionID int,\n",
    "    StateAb text,\n",
    "    PartyAb text,\n",
    "    LNCVotes int,\n",
    "    LNCPercentage float,\n",
    "    ALPVotes int,\n",
    "    ALPPercentage float,\n",
    "    TotalVotes int,\n",
    "    Swing float,\n",
    "    Election_Year int)\n",
    "    \"\"\"\n",
    "\n",
    "exe_turnout = \"\"\"\n",
    "CREATE TABLE HouseTurnout (\n",
    "    DivisionID int,\n",
    "    DivisionNm text,\n",
    "    StateAb text,\n",
    "    Enrolment int,\n",
    "    Turnout int,\n",
    "    TurnoutPercentage float,\n",
    "    TurnoutSwing float,\n",
    "    Election_Year int)\n",
    "    \"\"\"\n",
    "exe_votecount = \"\"\"\n",
    "CREATE TABLE HouseVotesCounted (\n",
    "    DivisionID int,\n",
    "    DivisionNm text,\n",
    "    StateAb text,\n",
    "    Enrolment int,\n",
    "    OrdinaryVotes int,\n",
    "    AbsentVotes int,\n",
    "    ProvisionalVotes int,\n",
    "    PrePollVotes int,\n",
    "    PostalVotes int,\n",
    "    TotalVotes int,\n",
    "    TotalPercentage float,\n",
    "    Election_Year int)\n",
    "    \"\"\"\n",
    "\n",
    "#Making a list of the queries for later\n",
    "exe_list = [exe_enrolment,exe_membelect,exe_tpp,exe_turnout,exe_votecount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the file name categories and years manually so they can be uploaded corresponding to each table\n",
    "election_years = ['2019','2016','2013','2010','2007','2004']\n",
    "file_names = ['GeneralEnrolmentByDivisionDownload','HouseMembersElectedDownload','HouseTppByDivisionDownload',\n",
    "              'HouseTurnoutByDivisionDownload','HouseVotesCountedByDivisionDownload']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going through all the files in my directory and uploading them corresponding to each table\n",
    "election_years = ['2019','2016','2013','2010','2007','2004']\n",
    "for exes,names in zip(exe_list,file_names):\n",
    "    #Executes the query in the pre defined query list\n",
    "    cur.execute(exes)\n",
    "    for year in election_years:\n",
    "        with open('./Database/AEC/' + names + '-' + year + '.csv', 'r') as f:\n",
    "            next(f)\n",
    "            #Uploads the data to the corresponding table name\n",
    "            cur.copy_from(f, names.replace('Download','').replace('ByDivision',''), sep=',')\n",
    "            conn.commit()\n",
    "    #Finalises the changes and uploads them to the database\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first five tables have now been uploaded into SQL, a relatively simple and straightforward process. With the Census data, this process will become far more complex not only because of the amount of CSVs, but also how messy the data is.\n",
    "\n",
    "## Cleaning and uploading the Census data:\n",
    "\n",
    "* Now for the hard part. For this section I will be importing a lot of functions that I have written from a module I wrote. It simply contains a series of functions that will clean the CSVs and upload them to the PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CED</th>\n",
       "      <th>CED_State</th>\n",
       "      <th>DivisionID</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>adelaide</td>\n",
       "      <td>SA</td>\n",
       "      <td>179</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>aston</td>\n",
       "      <td>VIC</td>\n",
       "      <td>197</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ballarat</td>\n",
       "      <td>VIC</td>\n",
       "      <td>198</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>banks</td>\n",
       "      <td>NSW</td>\n",
       "      <td>103</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>barker</td>\n",
       "      <td>SA</td>\n",
       "      <td>180</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>throsby</td>\n",
       "      <td>NSW</td>\n",
       "      <td>150</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>wakefield</td>\n",
       "      <td>SA</td>\n",
       "      <td>191</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>kalgoorlie</td>\n",
       "      <td>WA</td>\n",
       "      <td>241</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>lowe</td>\n",
       "      <td>NSW</td>\n",
       "      <td>129</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>prospect</td>\n",
       "      <td>NSW</td>\n",
       "      <td>142</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CED CED_State  DivisionID  Year\n",
       "0      adelaide        SA         179  2019\n",
       "1         aston       VIC         197  2019\n",
       "2      ballarat       VIC         198  2019\n",
       "3         banks       NSW         103  2019\n",
       "4        barker        SA         180  2019\n",
       "..          ...       ...         ...   ...\n",
       "442     throsby       NSW         150  2013\n",
       "443   wakefield        SA         191  2013\n",
       "681  kalgoorlie        WA         241  2007\n",
       "693        lowe       NSW         129  2007\n",
       "728    prospect       NSW         142  2007\n",
       "\n",
       "[163 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The reference table is so each of the electorate rows can be assigned a unique DivisionID so they can be aligned\n",
    "ref_table = pd.read_csv('./Database/AEC/All_reftable.csv')\n",
    "ref_table.rename(columns={'DivisionNm':'CED','StateAb':'CED_State'},inplace=True)\n",
    "ref_table.drop_duplicates(subset='CED',inplace=True)\n",
    "ref_table['CED'] = ref_table['CED'].apply(lambda x:x.lower())\n",
    "ref_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The reference table above contains every electorate from every election year since 2004 and contains their unique division ID. The purpose of this table is so I can align the electorate names from the Census data to the divisionID from the AEC. This will make it far easier to join later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Iterates through the entire directory, and outputs it to the 'clean' directory\n",
    "directory = './Database/ABS/table_builder'\n",
    "for filename in os.listdir(directory):\n",
    "    if '.DS_Store' not in filename:\n",
    "        #Applies the CSV cleaner function to every file in the table builder directory (refer to docstring)\n",
    "        wr.csv_cleaner(os.path.join(directory,filename),filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Defining the directory that we want to upload to SQL from\n",
    "#Clean directory is the cleaned files after going through wr.csv_cleaner\n",
    "clean_directory = './Database/ABS/clean_table_builder/'\n",
    "\n",
    "#This is all the category names for every file in the clean directory\n",
    "categories = set([filename.split('_')[1] for filename in os.listdir(clean_directory) if '.DS_Store' not in filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading the CSVs into SQL for all files in the clean directory\n",
    "for category in categories:\n",
    "    wr.sql_upload(category,clean_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The process of getting the raw CSVs was relatively straight forward. You take the raw CSVs, put them through the CSV cleaner function, output them to a new directory, then upload them to its respective SQL table. \n",
    "\n",
    "\n",
    "* The underlying functions were much more difficult and complicated, and took a lot of patience to get right. But for now, the actual Census data is now in the SQL data base.\n",
    "\n",
    "## Interpolating new census data:\n",
    "\n",
    "* With the 2006, 2011, and 2016 Census' now in my SQL database, it is time to interpolate more data for the 2004, 2007, 2010, 2013, and 2019 elections. The only aligning Census and election year was 2016, so I will not have to interpolate for that year. \n",
    "\n",
    "\n",
    "* The interpolation process is a lot more difficult than the simple CSV cleaning process above. I will run through the steps to give an overview of how it will work:\n",
    "\n",
    "    1. From the original uncleaned table builder files, run the 'csv_interpolate_clean' function. This custom function is a modification of the original CSV cleaner function, with the difference being it does not take a proportion.\n",
    "    \n",
    "    2. Once the files have gone through the preliminary clean, it is time to put them through the 'interpolater'. This is function works by taking in a specified year that you want to interpolate to (e.g. 2007), and taking in the upper bound and lower bound Census years. The reason for taking in the upper/lower bound is so it can calculate the growth rate over 5 years, and calculate a Carried Annual Growth Rate on a year basis. For the 2007 example, it will calculate the 5 year growth rate from 2006-2011, calculate the CAGR per year, then apply that for one year growth for 2007. For 2010, it will apply the CAGR from 2006-2011 and apply that for three years growth (CAGR^3).\n",
    "    \n",
    "    3. The interpolater, once it has calculated the CAGR between the lower and upper bound Census years, will apply the growth to every single cell in every single CSV. It is now left with a series of counts and not proportions.\n",
    "    \n",
    "    4. Finally, the interpolater will take the interpolated counts and transform them into proportions, identical to the original Census years' cleaned CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the original table_builder csvs to be set up so they can be interpolated with non census years later on\n",
    "#Defining the directory from where we will be cleaning the files\n",
    "new_dir = './Database/ABS/table_builder/'\n",
    "for file in os.listdir(new_dir):\n",
    "    #Applying the interpolate_clean method from the wrangler class (refer to docstring)\n",
    "    wr.csv_interpolate_clean(os.path.join(new_dir,file),file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the directory with the ready to be interpolated files\n",
    "interpolate_dir = './Database/ABS/to_interpolate/'\n",
    "\n",
    "#Separating the files into their respective census years, this will make it easier to organise\n",
    "census_06 = [file for file in os.listdir(interpolate_dir) if '2006' in file]\n",
    "census_11 = [file for file in os.listdir(interpolate_dir) if '2011' in file]\n",
    "census_16 = [file for file in os.listdir(interpolate_dir) if '2016' in file]\n",
    "\n",
    "#Setting a list of the 27 category names of the census\n",
    "names = set([name.split('_')[1] for name in os.listdir(interpolate_dir) if '.DS_Store' not in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now filtering each file based on name and census year\n",
    "for name in names:\n",
    "    c_06 = [file for file in census_06 if name == file.split('_')[1]][0]\n",
    "    c_11 = [file for file in census_11 if name == file.split('_')[1]][0]\n",
    "    c_16 = [file for file in census_16 if name == file.split('_')[1]][0]\n",
    "    #Interpolating for all the election years (bar 2016 since that is already aligned)\n",
    "    for year in ['2004','2007','2010','2013','2019']:\n",
    "        #Applying the interpolater based on the lower/upper bounds if it is less than 2011\n",
    "        if float(year) < 2011:\n",
    "            wr.interpolater(c_06,c_11,interpolate_dir,year)\n",
    "        #Applying it with the low/up bounds being 2011 and 2016\n",
    "        else:\n",
    "            wr.interpolater(c_11,c_16,interpolate_dir,year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that the interpolation is done, it is simply a matter of uploading the interpolated CSVs into their respective SQL tables on my database. \n",
    "\n",
    "\n",
    "* To do this, I'm using a slightly modified version of my original SQL uploader function. The original SQL uploader function had to construct the empty table first before inputting the CSV data. With this one, since the empty SQL tables are already built, I just need to simply upload the data corresponding to the correct category name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the interpolated directory\n",
    "interpolated_directory = './Database/ABS/interpolated/'\n",
    "#Getting a list of category names from the directory\n",
    "categories_int = set([filename.split('_')[1] for filename in os.listdir(interpolated_directory) if '.DS_Store' not in filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading the interpolated data to the database with the already defined tables\n",
    "for category in categories_int:\n",
    "    #Refer to docstring for more info on the wr.sql_upload_int method\n",
    "    wr.sql_upload_int(category,interpolated_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The database is now complete, there should be demographics and election data for every single election year spanning 2004-2019. I am estimating about 900 rows of data, approximately 150 per election year. The data frame will be quite wide and relatively shallow, but given the amount of data I had at my disposal, I am happy with the results so far.\n",
    "\n",
    "## Joining tables to construct the final dataframe:\n",
    "\n",
    "* With my database now fully constructed and easily accessible on SQL, it's time to join them all together to make a workable dataframe for my Exploratory Data Analysis and modeling. \n",
    "\n",
    "\n",
    "* In all, I will be joining 32 different SQL tables together. They will be joined by both their respective unique DivisionID and their respective election year. So the overall join will be on two columns over 32 tables.\n",
    "\n",
    "\n",
    "* This was a daunting task, but once I found the correct syntax, I could easily write a function that'd generate an extremely long SQL query to join every table on those two columns.\n",
    "\n",
    "\n",
    "* The process was as follow:\n",
    "\n",
    "    1. Separate the election data and census data. I will be making one table for all the AEC data, and one for all the ABS data via joining them through SQL.\n",
    "    2. With my SQL joiner function (refer to docstring), I will iterate through every single table name, and do an inner join on both DivisionID and election year. This will make one big table containing all the census data and interpolated census data for every election since 2004. This process will also be the same for the election data\n",
    "    3. Once I have those two tables I will read them into Pandas and do a simple pandas merge on the same two columns. \n",
    "    4. Done! After that merge, the table is constructed and I have my finished data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing all the table names from my SQL database\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"\"\"SELECT table_name FROM information_schema.tables\n",
    "       WHERE table_schema = 'public'\"\"\")\n",
    "    \n",
    "#Putting them into a list for use below\n",
    "sql_tables = [table[0] for table in cursor.fetchall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting the tables into election and census data, this will make it much easier to join\n",
    "election_tables = ['housetpp','housememberselected','houseturnout','housevotescounted','generalenrolment']\n",
    "\n",
    "census_tables = [table for table in sql_tables if table not in election_tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the wr.sql_join method I am making a query that joins all the columns from every single table\n",
    "election_q = wr.sql_join(election_tables,'election')\n",
    "#Reading it into a dataframe\n",
    "elec = pd.read_sql(election_q,con=conn)\n",
    "#Removing duplicate columns\n",
    "elec = elec.loc[:,~elec.columns.duplicated()]\n",
    "#Making a new 'year' column for easier joining\n",
    "elec['year'] = elec['election_year']\n",
    "\n",
    "census_q = wr.sql_join(census_tables,'census')\n",
    "#Reading it into a dataframe\n",
    "cens = pd.read_sql(census_q,con=conn)\n",
    "#Removing duplicate columns\n",
    "cens = cens.loc[:,~cens.columns.duplicated()]\n",
    "#Making a new 'year' column for easier joining\n",
    "cens['year'] = cens['census_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>divisionnm</th>\n",
       "      <th>divisionid</th>\n",
       "      <th>stateab</th>\n",
       "      <th>partyab</th>\n",
       "      <th>lncvotes</th>\n",
       "      <th>lncpercentage</th>\n",
       "      <th>alpvotes</th>\n",
       "      <th>alppercentage</th>\n",
       "      <th>totalvotes</th>\n",
       "      <th>swing</th>\n",
       "      <th>...</th>\n",
       "      <th>employer_government_includes_defence_housing_authority_pct</th>\n",
       "      <th>employer_other_employer_pct</th>\n",
       "      <th>housing_co_operative_community_church_group_pct</th>\n",
       "      <th>owned_outright_pct</th>\n",
       "      <th>owned_with_a_mortgage_pct</th>\n",
       "      <th>being_purchased_under_a_rent_buy_scheme_pct</th>\n",
       "      <th>rented_pct</th>\n",
       "      <th>being_occupied_rent_free_pct</th>\n",
       "      <th>being_occupied_under_a_life_tenure_scheme_pct</th>\n",
       "      <th>other_tenure_type_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>179</td>\n",
       "      <td>SA</td>\n",
       "      <td>ALP</td>\n",
       "      <td>44819</td>\n",
       "      <td>41.82</td>\n",
       "      <td>62362</td>\n",
       "      <td>58.18</td>\n",
       "      <td>107181</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062538</td>\n",
       "      <td>0.657678</td>\n",
       "      <td>1.422633</td>\n",
       "      <td>23.297441</td>\n",
       "      <td>23.705533</td>\n",
       "      <td>0.057609</td>\n",
       "      <td>32.926855</td>\n",
       "      <td>0.761181</td>\n",
       "      <td>0.880346</td>\n",
       "      <td>0.323751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Aston</td>\n",
       "      <td>197</td>\n",
       "      <td>VIC</td>\n",
       "      <td>LP</td>\n",
       "      <td>60180</td>\n",
       "      <td>60.13</td>\n",
       "      <td>39910</td>\n",
       "      <td>39.87</td>\n",
       "      <td>100090</td>\n",
       "      <td>2.72</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175370</td>\n",
       "      <td>0.306101</td>\n",
       "      <td>28.943640</td>\n",
       "      <td>36.751440</td>\n",
       "      <td>0.068675</td>\n",
       "      <td>20.402274</td>\n",
       "      <td>0.501577</td>\n",
       "      <td>0.859395</td>\n",
       "      <td>0.459208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>198</td>\n",
       "      <td>VIC</td>\n",
       "      <td>ALP</td>\n",
       "      <td>40068</td>\n",
       "      <td>39.02</td>\n",
       "      <td>62615</td>\n",
       "      <td>60.98</td>\n",
       "      <td>102683</td>\n",
       "      <td>-3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042602</td>\n",
       "      <td>0.296630</td>\n",
       "      <td>0.622956</td>\n",
       "      <td>27.043727</td>\n",
       "      <td>29.219009</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>21.820657</td>\n",
       "      <td>0.660488</td>\n",
       "      <td>0.291292</td>\n",
       "      <td>0.396658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Banks</td>\n",
       "      <td>103</td>\n",
       "      <td>NSW</td>\n",
       "      <td>LP</td>\n",
       "      <td>51609</td>\n",
       "      <td>56.26</td>\n",
       "      <td>40121</td>\n",
       "      <td>43.74</td>\n",
       "      <td>91730</td>\n",
       "      <td>4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078412</td>\n",
       "      <td>0.338183</td>\n",
       "      <td>0.662237</td>\n",
       "      <td>29.647115</td>\n",
       "      <td>28.422682</td>\n",
       "      <td>0.039335</td>\n",
       "      <td>28.825386</td>\n",
       "      <td>0.673301</td>\n",
       "      <td>0.337973</td>\n",
       "      <td>0.284562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Barker</td>\n",
       "      <td>180</td>\n",
       "      <td>SA</td>\n",
       "      <td>LP</td>\n",
       "      <td>72851</td>\n",
       "      <td>68.94</td>\n",
       "      <td>32815</td>\n",
       "      <td>31.06</td>\n",
       "      <td>105666</td>\n",
       "      <td>5.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297276</td>\n",
       "      <td>1.336800</td>\n",
       "      <td>0.652348</td>\n",
       "      <td>27.535345</td>\n",
       "      <td>24.324854</td>\n",
       "      <td>0.039489</td>\n",
       "      <td>20.001748</td>\n",
       "      <td>1.301663</td>\n",
       "      <td>0.855056</td>\n",
       "      <td>0.437370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>Watson</td>\n",
       "      <td>251</td>\n",
       "      <td>NSW</td>\n",
       "      <td>ALP</td>\n",
       "      <td>24029</td>\n",
       "      <td>34.86</td>\n",
       "      <td>44899</td>\n",
       "      <td>65.14</td>\n",
       "      <td>68928</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065619</td>\n",
       "      <td>0.460131</td>\n",
       "      <td>0.398981</td>\n",
       "      <td>32.446206</td>\n",
       "      <td>21.630599</td>\n",
       "      <td>0.294440</td>\n",
       "      <td>29.506493</td>\n",
       "      <td>0.728206</td>\n",
       "      <td>0.177017</td>\n",
       "      <td>0.229964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>Wentworth</td>\n",
       "      <td>152</td>\n",
       "      <td>NSW</td>\n",
       "      <td>LP</td>\n",
       "      <td>40847</td>\n",
       "      <td>55.48</td>\n",
       "      <td>32777</td>\n",
       "      <td>44.52</td>\n",
       "      <td>73624</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088357</td>\n",
       "      <td>0.296624</td>\n",
       "      <td>0.296051</td>\n",
       "      <td>21.849481</td>\n",
       "      <td>15.382495</td>\n",
       "      <td>0.069304</td>\n",
       "      <td>31.466465</td>\n",
       "      <td>0.898127</td>\n",
       "      <td>0.266259</td>\n",
       "      <td>0.269678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>Werriwa</td>\n",
       "      <td>153</td>\n",
       "      <td>NSW</td>\n",
       "      <td>ALP</td>\n",
       "      <td>31570</td>\n",
       "      <td>40.69</td>\n",
       "      <td>46012</td>\n",
       "      <td>59.31</td>\n",
       "      <td>77582</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171476</td>\n",
       "      <td>0.272215</td>\n",
       "      <td>0.380005</td>\n",
       "      <td>20.458323</td>\n",
       "      <td>39.271324</td>\n",
       "      <td>0.215366</td>\n",
       "      <td>25.780885</td>\n",
       "      <td>0.513531</td>\n",
       "      <td>0.288302</td>\n",
       "      <td>0.190053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>Wide Bay</td>\n",
       "      <td>178</td>\n",
       "      <td>QLD</td>\n",
       "      <td>NP</td>\n",
       "      <td>51489</td>\n",
       "      <td>62.89</td>\n",
       "      <td>30388</td>\n",
       "      <td>37.11</td>\n",
       "      <td>81877</td>\n",
       "      <td>2.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210800</td>\n",
       "      <td>0.645038</td>\n",
       "      <td>0.653188</td>\n",
       "      <td>32.090607</td>\n",
       "      <td>23.402774</td>\n",
       "      <td>0.188344</td>\n",
       "      <td>21.027154</td>\n",
       "      <td>1.137351</td>\n",
       "      <td>0.345970</td>\n",
       "      <td>1.008388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>Wills</td>\n",
       "      <td>234</td>\n",
       "      <td>VIC</td>\n",
       "      <td>ALP</td>\n",
       "      <td>27155</td>\n",
       "      <td>33.10</td>\n",
       "      <td>54893</td>\n",
       "      <td>66.90</td>\n",
       "      <td>82048</td>\n",
       "      <td>3.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015296</td>\n",
       "      <td>0.428938</td>\n",
       "      <td>0.327555</td>\n",
       "      <td>35.753278</td>\n",
       "      <td>24.058467</td>\n",
       "      <td>0.182108</td>\n",
       "      <td>23.895555</td>\n",
       "      <td>0.793937</td>\n",
       "      <td>0.101821</td>\n",
       "      <td>0.263438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>876 rows × 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    divisionnm  divisionid stateab partyab  lncvotes  lncpercentage  alpvotes  \\\n",
       "0     Adelaide         179      SA     ALP     44819          41.82     62362   \n",
       "1        Aston         197     VIC      LP     60180          60.13     39910   \n",
       "2     Ballarat         198     VIC     ALP     40068          39.02     62615   \n",
       "3        Banks         103     NSW      LP     51609          56.26     40121   \n",
       "4       Barker         180      SA      LP     72851          68.94     32815   \n",
       "..         ...         ...     ...     ...       ...            ...       ...   \n",
       "871     Watson         251     NSW     ALP     24029          34.86     44899   \n",
       "872  Wentworth         152     NSW      LP     40847          55.48     32777   \n",
       "873    Werriwa         153     NSW     ALP     31570          40.69     46012   \n",
       "874   Wide Bay         178     QLD      NP     51489          62.89     30388   \n",
       "875      Wills         234     VIC     ALP     27155          33.10     54893   \n",
       "\n",
       "     alppercentage  totalvotes  swing  ...  \\\n",
       "0            58.18      107181   0.12  ...   \n",
       "1            39.87      100090   2.72  ...   \n",
       "2            60.98      102683  -3.62  ...   \n",
       "3            43.74       91730   4.82  ...   \n",
       "4            31.06      105666   5.07  ...   \n",
       "..             ...         ...    ...  ...   \n",
       "871          65.14       68928   2.17  ...   \n",
       "872          44.52       73624  -2.38  ...   \n",
       "873          59.31       77582  -0.82  ...   \n",
       "874          37.11       81877   2.99  ...   \n",
       "875          66.90       82048   3.67  ...   \n",
       "\n",
       "     employer_government_includes_defence_housing_authority_pct  \\\n",
       "0                                             0.062538            \n",
       "1                                             0.000000            \n",
       "2                                             0.042602            \n",
       "3                                             0.078412            \n",
       "4                                             0.297276            \n",
       "..                                                 ...            \n",
       "871                                           0.065619            \n",
       "872                                           0.088357            \n",
       "873                                           0.171476            \n",
       "874                                           0.210800            \n",
       "875                                           0.015296            \n",
       "\n",
       "     employer_other_employer_pct  \\\n",
       "0                       0.657678   \n",
       "1                       0.175370   \n",
       "2                       0.296630   \n",
       "3                       0.338183   \n",
       "4                       1.336800   \n",
       "..                           ...   \n",
       "871                     0.460131   \n",
       "872                     0.296624   \n",
       "873                     0.272215   \n",
       "874                     0.645038   \n",
       "875                     0.428938   \n",
       "\n",
       "    housing_co_operative_community_church_group_pct owned_outright_pct  \\\n",
       "0                                          1.422633          23.297441   \n",
       "1                                          0.306101          28.943640   \n",
       "2                                          0.622956          27.043727   \n",
       "3                                          0.662237          29.647115   \n",
       "4                                          0.652348          27.535345   \n",
       "..                                              ...                ...   \n",
       "871                                        0.398981          32.446206   \n",
       "872                                        0.296051          21.849481   \n",
       "873                                        0.380005          20.458323   \n",
       "874                                        0.653188          32.090607   \n",
       "875                                        0.327555          35.753278   \n",
       "\n",
       "    owned_with_a_mortgage_pct  being_purchased_under_a_rent_buy_scheme_pct  \\\n",
       "0                   23.705533                                     0.057609   \n",
       "1                   36.751440                                     0.068675   \n",
       "2                   29.219009                                     0.036482   \n",
       "3                   28.422682                                     0.039335   \n",
       "4                   24.324854                                     0.039489   \n",
       "..                        ...                                          ...   \n",
       "871                 21.630599                                     0.294440   \n",
       "872                 15.382495                                     0.069304   \n",
       "873                 39.271324                                     0.215366   \n",
       "874                 23.402774                                     0.188344   \n",
       "875                 24.058467                                     0.182108   \n",
       "\n",
       "     rented_pct  being_occupied_rent_free_pct  \\\n",
       "0     32.926855                      0.761181   \n",
       "1     20.402274                      0.501577   \n",
       "2     21.820657                      0.660488   \n",
       "3     28.825386                      0.673301   \n",
       "4     20.001748                      1.301663   \n",
       "..          ...                           ...   \n",
       "871   29.506493                      0.728206   \n",
       "872   31.466465                      0.898127   \n",
       "873   25.780885                      0.513531   \n",
       "874   21.027154                      1.137351   \n",
       "875   23.895555                      0.793937   \n",
       "\n",
       "     being_occupied_under_a_life_tenure_scheme_pct  other_tenure_type_pct  \n",
       "0                                         0.880346               0.323751  \n",
       "1                                         0.859395               0.459208  \n",
       "2                                         0.291292               0.396658  \n",
       "3                                         0.337973               0.284562  \n",
       "4                                         0.855056               0.437370  \n",
       "..                                             ...                    ...  \n",
       "871                                       0.177017               0.229964  \n",
       "872                                       0.266259               0.269678  \n",
       "873                                       0.288302               0.190053  \n",
       "874                                       0.345970               1.008388  \n",
       "875                                       0.101821               0.263438  \n",
       "\n",
       "[876 rows x 290 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now with the two tables defined, I will merge them on an inner join with aligning division id and year\n",
    "demo_df = elec.merge(cens,on=['divisionid','year'])\n",
    "#The finished product, voila!\n",
    "demo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputting the finished product to a csv file which will be read in for more cleaning later\n",
    "demo_df.to_csv('./Database/CED_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The finished product, is a 876 row deep and 290 column wide data frame. The excellent thing about constructing your own dataframe from the ground up is that there are no missing values, and that all the values that will be modeled are already on the exact same scale. Having everything in proportion means I do not have to standardise any of the figures.\n",
    "\n",
    "\n",
    "* The amount of time and work that was put in to making this dataframe will be worth it once it comes to the modeling, as there will be very little I need to do to make it workable. \n",
    "\n",
    "\n",
    "* Over the next two sections I will do a little bit of cleaning for labeling and readability, but the dataframe in the form its in now, needs very little work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
